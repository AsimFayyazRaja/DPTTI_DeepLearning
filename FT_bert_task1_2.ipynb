{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FT_bert_task1_2.ipynb","provenance":[{"file_id":"1gThBv4HFNiNMW374vM9jhl2ZImM_i5Hm","timestamp":1645022630902}],"collapsed_sections":[],"authorship_tag":"ABX9TyME3kibizvIeuoBsaTVL0da"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nq9NaF92w57r","executionInfo":{"status":"ok","timestamp":1645085074813,"user_tz":-300,"elapsed":3698,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"adcfb587-6b33-4ee4-8bb8-046963ea0c30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd drive/MyDrive/Sapienza/DL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guU_uybWw8ET","executionInfo":{"status":"ok","timestamp":1645085074814,"user_tz":-300,"elapsed":8,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"6cf35566-6f60-4c1f-dc59-5bddc49fb365"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Sapienza/DL\n"]}]},{"cell_type":"code","source":["import pickle\n","import pandas as pd\n","import numpy as np\n","import json\n","import itertools\n","import gensim\n","path=\"SEMEVAL-2021-task6-corpus/data/\"\n"],"metadata":{"id":"-57aU4o-w8Gq","executionInfo":{"status":"ok","timestamp":1645085075534,"user_tz":-300,"elapsed":723,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["with open('training_task1_labels.pkl', 'rb') as handle:\n","    train_labs = pickle.load(handle)\n","\n","with open('original_training_task1_features.pkl', 'rb') as handle:\n","    train_feats = pickle.load(handle)\n","\n","with open('training_task1_all_labels.pkl', 'rb') as handle:\n","    all_labels = pickle.load(handle)\n","\n","with open('dev_task1_labels.pkl', 'rb') as handle:\n","    dev_labs = pickle.load(handle)\n","\n","with open('original_dev_task1_features.pkl', 'rb') as handle:\n","    dev_feats = pickle.load(handle)\n","\n","with open('test_task1_labels.pkl', 'rb') as handle:\n","    test_labs = pickle.load(handle)\n","\n","with open('original_test_task1_features.pkl', 'rb') as handle:\n","    test_feats = pickle.load(handle)\n"],"metadata":{"id":"nyW9q6Hcw8Is","executionInfo":{"status":"ok","timestamp":1645085075534,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip3 install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eISR_1igAe7M","executionInfo":{"status":"ok","timestamp":1645084012361,"user_tz":-300,"elapsed":9811,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"1f62682b-683e-42d4-c574-e9ce091b6970"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 43.4 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 32.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 46.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"]}]},{"cell_type":"code","source":["new_labs=[]\n","\n","for lab in train_labs:\n","  new_labs.append(np.argmax(lab))\n"],"metadata":{"id":"m1H9jOeboV7G","executionInfo":{"status":"ok","timestamp":1645085079759,"user_tz":-300,"elapsed":537,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["new_labs1=[]\n","\n","for lab in dev_labs:\n","  new_labs1.append(np.argmax(lab))\n"],"metadata":{"id":"nhZw1mOUoXoY","executionInfo":{"status":"ok","timestamp":1645085862196,"user_tz":-300,"elapsed":2,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yLgKbidVoXq_","executionInfo":{"status":"ok","timestamp":1645085080325,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BqhijKFEoXtJ","executionInfo":{"status":"ok","timestamp":1645085080325,"user_tz":-300,"elapsed":2,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","import time\n","import datetime\n","import random\n","from google.colab import drive\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.metrics import accuracy_score\n","\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoConfig\n","from transformers import AdamW\n","from transformers import AutoTokenizer\n","from transformers import get_linear_schedule_with_warmup\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.colors\n","import seaborn as sns\n","import IPython\n","\n","import torch\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"],"metadata":{"id":"4AMwnYj4EhFd","executionInfo":{"status":"ok","timestamp":1645085095066,"user_tz":-300,"elapsed":8760,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miPyTvy_EhHr","executionInfo":{"status":"ok","timestamp":1645085095066,"user_tz":-300,"elapsed":4,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"945b0c8e-1cfe-4669-d6e0-982f234571a3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"]}]},{"cell_type":"code","source":["texts=train_feats\n","label_names=all_labels\n","labels=new_labs"],"metadata":{"id":"d6DMmzNVEhKJ","executionInfo":{"status":"ok","timestamp":1645085870369,"user_tz":-300,"elapsed":357,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"0JDMx0tsnhDq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645085096276,"user_tz":-300,"elapsed":349,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"5244ecd4-e7de-4ac0-b315-350a0cba9918"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Feb 17 08:04:56 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P8    27W / 149W |      3MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"pzcvm-JVnhGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"z8xwOTvEnhI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bxymP3GXnhL0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"bert-base-uncased\"\n","MAX_INPUT_LENGTH = 64"],"metadata":{"id":"uh43US5WEhMR","executionInfo":{"status":"ok","timestamp":1645085874483,"user_tz":-300,"elapsed":562,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["print(f\"Loading {model_name} tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3kaYf-5EhO3","executionInfo":{"status":"ok","timestamp":1645085876357,"user_tz":-300,"elapsed":1327,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"bb0aff1a-fa77-4bbb-b80c-dcaccb42d0ee"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading bert-base-uncased tokenizer...\n"]}]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","for text in texts:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,            \n","                        add_special_tokens = True,\n","                        max_length = MAX_INPUT_LENGTH,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt')\n","  \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1-1yxZBEhRI","executionInfo":{"status":"ok","timestamp":1645085878158,"user_tz":-300,"elapsed":545,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"f2205000-2e86-44f8-de86-9a2f833918a7"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels, dtype=torch.long)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])\n","print('Attention Mask:', attention_masks[0]) # 1 for all text tokens, 0 for all padding tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fBqT0Mx_EhTl","executionInfo":{"status":"ok","timestamp":1645085879808,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"cdd7cdff-4814-4e1d-b115-2504feed21fb"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  THERE ARE ONLY TWO GENDERS FEMALE  MALE\n","Token IDs: tensor([101, 100, 100, 100, 100, 100, 100, 100, 102,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","          0,   0,   0,   0,   0,   0,   0,   0])\n","Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"]}]},{"cell_type":"code","source":["labels[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IW3kTrB2EhV7","executionInfo":{"status":"ok","timestamp":1645085111728,"user_tz":-300,"elapsed":547,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"26c43cdf-5e04-4d1d-9e1e-58899700fac8"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["train_dataset = TensorDataset(input_ids, attention_masks, labels)\n","print(len(train_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrMY998Iwopd","executionInfo":{"status":"ok","timestamp":1645085917308,"user_tz":-300,"elapsed":336,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"5344a85e-1f03-4a11-b0a8-9758a52c6962"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["688\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"zQLCJqQMwz1w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bv4LJql_wz4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for dev\n","texts=dev_feats\n","label_names=all_labels\n","labels=new_labs1"],"metadata":{"id":"3-_S5fFhwz6t","executionInfo":{"status":"ok","timestamp":1645085995995,"user_tz":-300,"elapsed":556,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","for text in texts:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,            \n","                        add_special_tokens = True,\n","                        max_length = MAX_INPUT_LENGTH,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt')\n","  \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WN0f0383wpDk","executionInfo":{"status":"ok","timestamp":1645085996640,"user_tz":-300,"elapsed":647,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"efcc7322-96c8-4738-8a46-15331db3a764"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels, dtype=torch.long)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])\n","print('Attention Mask:', attention_masks[0]) # 1 for all text tokens, 0 for all padding tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdyaUqlUxCDB","executionInfo":{"status":"ok","timestamp":1645085997401,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"8c4597c4-255c-4b38-e126-1fe257f3cbb6"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  *President* Biden? Please, no.\n","Token IDs: tensor([ 101, 1008,  100, 1008,  100, 1029,  100, 1010, 2053, 1012,  102,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0])\n","Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"]}]},{"cell_type":"code","source":["val_dataset = TensorDataset(input_ids, attention_masks, labels)\n","print(len(val_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8Of6PxqwpGO","executionInfo":{"status":"ok","timestamp":1645086001348,"user_tz":-300,"elapsed":508,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"53101c14-acc0-49e8-f6ef-2be2f19d2df1"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["63\n"]}]},{"cell_type":"code","source":["# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset,\n","            sampler = SequentialSampler(val_dataset),\n","            batch_size = batch_size\n","        )"],"metadata":{"id":"eP-SHTx4Ehas","executionInfo":{"status":"ok","timestamp":1645086024397,"user_tz":-300,"elapsed":368,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["print(f\"Loading {model_name} model...\")\n","config = AutoConfig.from_pretrained(model_name)\n","config.num_labels = len(label_names)\n","config.output_attentions = True\n","print(\"config\", config)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0Wmnt2OEhcv","executionInfo":{"status":"ok","timestamp":1645086027467,"user_tz":-300,"elapsed":380,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"106c21c8-0c49-4787-a311-8880d6917102"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading bert-base-uncased model...\n","config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_attentions\": true,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"GdGP-msGtY2P","executionInfo":{"status":"ok","timestamp":1645086301477,"user_tz":-300,"elapsed":494,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    config=config)\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojIAMe1yspg7","executionInfo":{"status":"ok","timestamp":1645086303772,"user_tz":-300,"elapsed":2300,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"f04b7eea-0ffe-4e61-84f1-0a1e4fe902e8"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=23, bias=True)\n",")"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5IIZS6to0Gk","executionInfo":{"status":"ok","timestamp":1645086305449,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"6c3778f9-29e6-4a1c-cd6b-d4a6eb12cdc7"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (30522, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                          (23, 768)\n","classifier.bias                                                (23,)\n"]}]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sr6spUAho0JF","executionInfo":{"status":"ok","timestamp":1645086306916,"user_tz":-300,"elapsed":3,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"3412516e-4063-4cdd-a4b3-b3862b0149b5"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Number of training epochs. The BERT authors recommend between 2 and 4. \n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"metadata":{"id":"-DwBUKOpo0Ln","executionInfo":{"status":"ok","timestamp":1645086307477,"user_tz":-300,"elapsed":2,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"6u6I6YQco0N_","executionInfo":{"status":"ok","timestamp":1645086307989,"user_tz":-300,"elapsed":1,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"CMV38hZ3pBB4","executionInfo":{"status":"ok","timestamp":1645086310629,"user_tz":-300,"elapsed":2,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","epochs=20\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        \n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits, attentions = model(input_ids=b_input_ids, \n","                                         attention_mask=b_input_mask, \n","                                         labels=b_labels,\n","                                         return_dict = False)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            (loss, logits, attentions) = model(input_ids=b_input_ids, \n","                                               attention_mask=b_input_mask,\n","                                               labels=b_labels,\n","                                               return_dict = False)\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        \n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V0mlzeHMpBEf","executionInfo":{"status":"ok","timestamp":1645086654190,"user_tz":-300,"elapsed":339377,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"271721f8-6fb5-4a2f-8274-49196c666b58"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.69\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.44\n","  Validation Loss: 2.26\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.36\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.17\n","  Validation took: 0:00:00\n","\n","======== Epoch 3 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.28\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.43\n","  Validation Loss: 2.13\n","  Validation took: 0:00:00\n","\n","======== Epoch 4 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 5 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:17\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 6 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 7 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.19\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 8 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 9 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 10 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:17\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 11 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:17\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 12 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 13 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 14 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 15 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 16 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 17 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.21\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 18 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:17\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 19 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","======== Epoch 20 / 20 ========\n","Training...\n","  Batch    40  of     43.    Elapsed: 0:00:15.\n","\n","  Average training loss: 2.20\n","  Training epoch took: 0:00:16\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 2.09\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:05:39 (h:mm:ss)\n"]}]},{"cell_type":"code","source":["# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table.\n","df_stats"],"metadata":{"id":"MVsCivs2pBIx","colab":{"base_uri":"https://localhost:8080/","height":708},"executionInfo":{"status":"ok","timestamp":1645086708466,"user_tz":-300,"elapsed":359,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"9ad89975-10fc-42f3-d0d3-b269ec9fdd1e"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7fcfd3c3-698f-47a4-8987-21373a46dc13\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>2.69</td>\n","      <td>2.26</td>\n","      <td>0.44</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.36</td>\n","      <td>2.17</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2.28</td>\n","      <td>2.13</td>\n","      <td>0.43</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:17</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2.19</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:17</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:17</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2.21</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:17</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2.20</td>\n","      <td>2.09</td>\n","      <td>0.38</td>\n","      <td>0:00:16</td>\n","      <td>0:00:00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fcfd3c3-698f-47a4-8987-21373a46dc13')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7fcfd3c3-698f-47a4-8987-21373a46dc13 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7fcfd3c3-698f-47a4-8987-21373a46dc13');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n","epoch                                                                         \n","1               2.69         2.26           0.44       0:00:16         0:00:00\n","2               2.36         2.17           0.38       0:00:16         0:00:00\n","3               2.28         2.13           0.43       0:00:16         0:00:00\n","4               2.21         2.09           0.38       0:00:16         0:00:00\n","5               2.20         2.09           0.38       0:00:17         0:00:00\n","6               2.21         2.09           0.38       0:00:16         0:00:00\n","7               2.19         2.09           0.38       0:00:16         0:00:00\n","8               2.20         2.09           0.38       0:00:16         0:00:00\n","9               2.20         2.09           0.38       0:00:16         0:00:00\n","10              2.21         2.09           0.38       0:00:17         0:00:00\n","11              2.20         2.09           0.38       0:00:17         0:00:00\n","12              2.20         2.09           0.38       0:00:16         0:00:00\n","13              2.21         2.09           0.38       0:00:16         0:00:00\n","14              2.20         2.09           0.38       0:00:16         0:00:00\n","15              2.21         2.09           0.38       0:00:16         0:00:00\n","16              2.20         2.09           0.38       0:00:16         0:00:00\n","17              2.21         2.09           0.38       0:00:16         0:00:00\n","18              2.20         2.09           0.38       0:00:17         0:00:00\n","19              2.20         2.09           0.38       0:00:16         0:00:00\n","20              2.20         2.09           0.38       0:00:16         0:00:00"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"c6DOSLDOEhfM","colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"status":"ok","timestamp":1645086716660,"user_tz":-300,"elapsed":609,"user":{"displayName":"Raja Asim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWWAM2a1zPM1lcj0bcWwfQYRuJYDumRJSSZ1-cug=s64","userId":"08441109681798812112"}},"outputId":"58ba1ac6-37d9-4b26-8d1f-3d05214453be"},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAuUAAAGaCAYAAACopj13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde1hU1f4G8HcGGEAYQLmNAQ6KgobIzUumpSAooqapeT3eMhVNS8uyTnc7djxmmVpaiT9NxSuCYqGmgpZlkpGaKZoYKHIRUWAAuQyzf394mNMEKoPAnhnez/Oc5zmsvffa39nwPL2zXHstiSAIAoiIiIiISDRSsQsgIiIiImrpGMqJiIiIiETGUE5EREREJDKGciIiIiIikTGUExERERGJjKGciIiIiEhkDOVEZLKysrLg4+OD1atXN7iP1157DT4+Po1Ylem61/P28fHBa6+9Vq8+Vq9eDR8fH2RlZTV6fXFxcfDx8cHJkycbvW8ioodlLnYBRNRy6BNujxw5And39yasxviUlZXh888/R2JiIm7cuIE2bdogODgYc+bMgZeXV736eOGFF3Dw4EHs2bMHXbp0qfMcQRAwYMAAFBcX4/jx47CysmrMj9GkTp48iZSUFEyZMgV2dnZil1NLVlYWBgwYgIkTJ+Ltt98WuxwiMiAM5UTUbJYtW6bz8y+//IIdO3Zg7NixCA4O1jnWpk2bh76fm5sbzp49CzMzswb38f777+O999576Foaw5tvvolvvvkGQ4cORc+ePZGfn4+kpCScOXOm3qF89OjROHjwIHbv3o0333yzznN++uknXL9+HWPHjm2UQH727FlIpc3zD7MpKSn49NNP8fTTT9cK5cOHD8eQIUNgYWHRLLUQEemDoZyIms3w4cN1fq6ursaOHTsQEBBQ69jflZSUwNbWVq/7SSQSWFpa6l3nXxlKgLtz5w4OHDiAvn374qOPPtK2z507F5WVlfXup2/fvmjbti327duHV199FTKZrNY5cXFxAO4G+MbwsL+DxmJmZvZQX9CIiJoS55QTkcEJDQ3FpEmTcP78eUyfPh3BwcF46qmnANwN5ytWrMAzzzyDXr16oWvXrggPD8fy5ctx584dnX7qmuP817bk5GSMGjUKfn5+6Nu3L/7zn/9ArVbr9FHXnPKaNpVKhXfeeQe9e/eGn58fxo0bhzNnztT6PLdv38brr7+OXr16ITAwEJMnT8b58+cxadIkhIaG1uuZSCQSSCSSOr8k1BWs70UqleLpp59GYWEhkpKSah0vKSnBt99+C29vb3Tr1k2v530vdc0p12g0+OKLLxAaGgo/Pz8MHToUCQkJdV6fnp6Od999F0OGDEFgYCD8/f0xcuRI7Nq1S+e81157DZ9++ikAYMCAAfDx8dH5/d9rTvmtW7fw3nvvoV+/fujatSv69euH9957D7dv39Y5r+b6EydOYP369QgLC0PXrl0xaNAgxMfH1+tZ6CMtLQ3PP/88evXqBT8/P0RGRmLdunWorq7WOS8nJwevv/46QkJC0LVrV/Tu3Rvjxo3TqUmj0WDjxo0YNmwYAgMDERQUhEGDBuGf//wnqqqqGr12ItIfR8qJyCBlZ2djypQpiIiIwMCBA1FWVgYAyMvLQ2xsLAYOHIihQ4fC3NwcKSkpiI6OxoULF7B+/fp69X/s2DFs3boV48aNw6hRo3DkyBH83//9H+zt7REVFVWvPqZPn442bdrg+eefR2FhITZs2ICZM2fiyJEj2lH9yspKTJs2DRcuXMDIkSPh5+eHixcvYtq0abC3t6/387CyssKIESOwe/dufP311xg6dGi9r/27kSNHYu3atYiLi0NERITOsW+++Qbl5eUYNWoUgMZ73n/373//G5s2bUKPHj0wdepUFBQUYPHixfDw8Kh1bkpKCk6dOoX+/fvD3d1d+68Gb775Jm7duoVZs2YBAMaOHYuSkhIcOnQIr7/+Olq3bg3g/u8yqFQqjB8/HpmZmRg1ahQeffRRXLhwAdu2bcNPP/2EXbt21foXmhUrVqC8vBxjx46FTCbDtm3b8Nprr6Fdu3a1pmE11G+//YZJkybB3NwcEydOhJOTE5KTk7F8+XKkpaVp/7VErVZj2rRpyMvLw4QJE+Dp6YmSkhJcvHgRp06dwtNPPw0AWLt2LVatWoWQkBCMGzcOZmZmyMrKQlJSEiorKw3mX4SIWjSBiEgku3fvFry9vYXdu3frtIeEhAje3t7Czp07a11TUVEhVFZW1mpfsWKF4O3tLZw5c0bbdu3aNcHb21tYtWpVrTZ/f3/h2rVr2naNRiMMGTJE6NOnj06/ixYtEry9vetse+edd3TaExMTBW9vb2Hbtm3ati1btgje3t7CmjVrdM6taQ8JCan1WeqiUqmEGTNmCF27dhUeffRR4ZtvvqnXdfcyefJkoUuXLkJeXp5O+5gxYwRfX1+hoKBAEISHf96CIAje3t7CokWLtD+np6cLPj4+wuTJkwW1Wq1tP3funODj4yN4e3vr/G5KS0tr3b+6ulr4xz/+IQQFBenUt2rVqlrX16j5e/vpp5+0bR9//LHg7e0tbNmyRefcmt/PihUral0/fPhwoaKiQtuem5sr+Pr6CgsWLKh1z7+reUbvvffefc8bO3as0KVLF+HChQvaNo1GI7zwwguCt7e38OOPPwqCIAgXLlwQvL29hS+//PK+/Y0YMUIYPHjwA+sjIvFw+goRGSQHBweMHDmyVrtMJtOO6qnVahQVFeHWrVt4/PHHAaDO6SN1GTBggM7qLhKJBL169UJ+fj5KS0vr1cfUqVN1fn7ssccAAJmZmdq25ORkmJmZYfLkyTrnPvPMM5DL5fW6j0ajwYsvvoi0tDTs378fTz75JBYuXIh9+/bpnPfWW2/B19e3XnPMR48ejerqauzZs0fblp6ejtOnTyM0NFT7om1jPe+/OnLkCARBwLRp03TmePv6+qJPnz61zm/VqpX2/1dUVOD27dsoLCxEnz59UFJSgitXruhdQ41Dhw6hTZs2GDt2rE772LFj0aZNGxw+fLjWNRMmTNCZMuTq6or27dsjIyOjwXX8VUFBAX799VeEhoaic+fO2naJRILZs2dr6wag/Rs6efIkCgoK7tmnra0t8vLycOrUqUapkYgaH6evEJFB8vDwuOdLeTExMdi+fTsuX74MjUajc6yoqKje/f+dg4MDAKCwsBA2NjZ691EzXaKwsFDblpWVBRcXl1r9yWQyuLu7o7i4+IH3OXLkCI4fP44PP/wQ7u7uWLlyJebOnYtXX30VarVaO0Xh4sWL8PPzq9cc84EDB8LOzg5xcXGYOXMmAGD37t0AoJ26UqMxnvdfXbt2DQDQoUOHWse8vLxw/PhxnbbS0lJ8+umn2L9/P3JycmpdU59neC9ZWVno2rUrzM11/3Nobm4OT09PnD9/vtY19/rbuX79eoPr+HtNANCxY8daxzp06ACpVKp9hm5uboiKisKXX36Jvn37okuXLnjssccQERGBbt26aa976aWX8Pzzz2PixIlwcXFBz5490b9/fwwaNEivdxKIqOkwlBORQbK2tq6zfcOGDVi6dCn69u2LyZMnw8XFBRYWFsjLy8Nrr70GQRDq1f/9VuF42D7qe3191byY2KNHDwB3A/2nn36K2bNn4/XXX4darUbnzp1x5swZLFmypF59WlpaYujQodi6dStSU1Ph7++PhIQEKBQKPPHEE9rzGut5P4yXX34ZR48exZgxY9CjRw84ODjAzMwMx44dw8aNG2t9UWhqzbW8Y30tWLAAo0ePxtGjR3Hq1CnExsZi/fr1eO655/DKK68AAAIDA3Ho0CEcP34cJ0+exMmTJ/H1119j7dq12Lp1q/YLKRGJh6GciIzK3r174ebmhnXr1umEo++++07Equ7Nzc0NJ06cQGlpqc5oeVVVFbKysuq1wU3N57x+/Tratm0L4G4wX7NmDaKiovDWW2/Bzc0N3t7eGDFiRL1rGz16NLZu3Yq4uDgUFRUhPz8fUVFROs+1KZ53zUjzlStX0K5dO51j6enpOj8XFxfj6NGjGD58OBYvXqxz7Mcff6zVt0Qi0buWP//8E2q1Wme0XK1WIyMjo85R8aZWM63q8uXLtY5duXIFGo2mVl0eHh6YNGkSJk2ahIqKCkyfPh3R0dF49tln4ejoCACwsbHBoEGDMGjQIAB3/wVk8eLFiI2NxXPPPdfEn4qIHsSwvu4TET2AVCqFRCLRGaFVq9VYt26diFXdW2hoKKqrq7Fp0yad9p07d0KlUtWrj379+gG4u+rHX+eLW1pa4uOPP4adnR2ysrIwaNCgWtMw7sfX1xddunRBYmIiYmJiIJFIaq1N3hTPOzQ0FBKJBBs2bNBZ3u/333+vFbRrvgj8fUT+xo0btZZEBP43/7y+02rCwsJw69atWn3t3LkTt27dQlhYWL36aUyOjo4IDAxEcnIyLl26pG0XBAFffvklACA8PBzA3dVj/r6koaWlpXZqUM1zuHXrVq37+Pr66pxDROLiSDkRGZWIiAh89NFHmDFjBsLDw1FSUoKvv/5arzDanJ555hls374dn3zyCa5evapdEvHAgQNQKpW11kWvS58+fTB69GjExsZiyJAhGD58OBQKBa5du4a9e/cCuBuwPvvsM3h5eWHw4MH1rm/06NF4//338f3336Nnz561RmCb4nl7eXlh4sSJ2LJlC6ZMmYKBAweioKAAMTEx6Ny5s848bltbW/Tp0wcJCQmwsrKCn58frl+/jh07dsDd3V1n/j4A+Pv7AwCWL1+OYcOGwdLSEp06dYK3t3edtTz33HM4cOAAFi9ejPPnz6NLly64cOECYmNj0b59+yYbQT537hzWrFlTq93c3BwzZ87EG2+8gUmTJmHixImYMGECnJ2dkZycjOPHj2Po0KHo3bs3gLtTm9566y0MHDgQ7du3h42NDc6dO4fY2Fj4+/trw3lkZCQCAgLQrVs3uLi4ID8/Hzt37oSFhQWGDBnSJJ+RiPRjmP8VIyK6h+nTp0MQBMTGxmLJkiVwdnbG4MGDMWrUKERGRopdXi0ymQxfffUVli1bhiNHjmD//v3o1q0bNm7ciDfeeAPl5eX16mfJkiXo2bMntm/fjvXr16Oqqgpubm6IiIjAs88+C5lMhrFjx+KVV16BXC5H375969XvsGHDsGzZMlRUVNR6wRNouuf9xhtvwMnJCTt37sSyZcvg6emJt99+G5mZmbVervzwww/x0UcfISkpCfHx8fD09MSCBQtgbm6O119/Xefc4OBgLFy4ENu3b8dbb70FtVqNuXPn3jOUy+VybNu2DatWrUJSUhLi4uLg6OiIcePGYd68eXrvIltfZ86cqXPlGplMhpkzZ8LPzw/bt2/HqlWrsG3bNpSVlcHDwwMLFy7Es88+qz3fx8cH4eHhSElJwb59+6DRaNC2bVvMmjVL57xnn30Wx44dw+bNm6FSqeDo6Ah/f3/MmjVLZ4UXIhKPRGiOt3SIiEhHdXU1HnvsMXTr1q3BG/AQEZHp4JxyIqImVtdo+Pbt21FcXFznutxERNTycPoKEVETe/PNN1FZWYnAwEDIZDL8+uuv+Prrr6FUKjFmzBixyyMiIgPA6StERE1sz549iImJQUZGBsrKyuDo6Ih+/frhxRdfhJOTk9jlERGRAWAoJyIiIiISGeeUExERERGJjKGciIiIiEhkfNHzv27fLoVG07wzeRwdbVFQUNKs9+T9Da8GIiIiahmkUglat7ap8xhD+X9pNEKzh/Ka+4qppd/fUGogIiKilo3TV4iIiIiIRCbaSPnZs2cRHx+PkydPIjs7Gw4ODggMDMT8+fOhVCrve21oaCiuX79e5zGlUolvv/22KUomIiIiImoSooXy6OhopKamIiIiAj4+PsjPz0dMTAxGjBiB2NhYeHl53fPaf/7znygtLdVpy87OxieffMLd8YiIiIjI6IgWyqdOnYrly5dDJpNp2yIjIzFs2DCsW7cOS5cuvee1YWFhtdrWrFkDABg2bFjjF0tERERE1IREC+VBQUG12jw9PdGpUyekp6fr3d/XX38Nd3f3OvslIiIiIjJkBrX6iiAIuHnzJjp37qzXdefPn0d6ejqioqKaqDIiIiJqqe7cKUVJSRGqq6vELoUMlJmZBWxt7WFtXfdyh/VhUKE8ISEBeXl5WLBggV7X7du3DwDw1FNPNUVZRERE1EJVVVVCpboNBwcnWFhYQiKRiF0SGRhBEFBVVYHCwpswN7eAhYXswRfVQSIIgkEs0pyeno4xY8bAx8cHW7ZsgVRav9UaNRoN+vfvD0dHR8THxzdxlURERNSSZGZehVRqAVtbO7FLIQNXUlIMjaYKSmW7Bl1vECPl+fn5mDVrFuzt7bFy5cp6B3IASElJQV5eHqZOnfpQNRQUlDT7JjLOznLk56ua9Z68v+HVQEREhqukpBSOjgqo1RqxSyEDZ2FhhYKCwvvmCqlUAkdH2zqPiR7KVSoVZsyYAZVKhW3btsHZ2Vmv6/ft2wepVIohQ4Y0UYWN78TvuYg7lo5bxRVoY2eJkf280NtXIXZZRERE9DcaTTWkUjOxyyAjIJWaQaOpbvD1oobyiooKREVFISMjAxs3bkSHDh30ur6yshLffvstevbsCVdX1yaqsnGd+D0XX+1PQ+V/v3EXFFfgq/1pAMBgTkREZIA4j5zq42H/Tuo/T6SRVVdXY/78+Th9+jRWrlyJgICAOs/Lzs6+5xKJx44dQ3FxsVGtTR53LF0byGtUqjWIO6b/MpBEREREZBpEGylfunQpkpKSEBISgsLCQuzdu1d7zMbGRrtB0KJFi5CSkoKLFy/W6mPfvn2QyWQYNGhQs9X9sAqKK/RqJyIiIjI2c+fOBAB8+umXzXqtMRMtlKel3Z2ykZycjOTkZJ1jbm5ude7a+VclJSU4evQo+vfvD7lc3mR1NjZHO8s6A7ijnaUI1RAREVFL0rdv93qdt2tXAtq2faSJq6G/MpglEcXWXKuv/H1OOQDIzKWYMrhzs88pF3vlEbHvbyg1EBGR4crNzYRCoRS7jEZz8GCizs87d25DXl4O5s17Saf9ySdDYG1t3eD7VFXd3WjJwsKiWa8V24P+Xgx69ZWWpiZ4xx1LR0FxBaRSiSiBnIiIiFqeQYMidX4+evQIiooKa7X/XXl5OaysrOp9n4cJ1MYYxhuDaC96tmS9fRX4cE4f/COiMzQaAQEdncQuiYiIiAjA3TndU6dOwPnz5zB79nSEhvZBTMxXAIDvvz+KV155EcOHRyAkpDfGjBmOjRujUV1dXauPmrnhAJCaegp9+3bHsWNJ2LgxGiNGDEZo6ON48cXZyMq61mjXAsDu3TvxzDPDERraBzNmTMaZM7/W6tMQcaRcRF7uDgCAq3kq+LRrLXI1RERE1Bxq9ispKK6Ao4HuV1JYeBuvvroAAwdGICJiCFxd79aXmPg1rK1bYezYiWjVyhq//HIK0dGfo7S0FM8//+ID+/3qq/WQSs0wYcJkqFTF2LZtM957702sW/dVo1wbHx+LFSuWISAgCGPHjkdOTg5ef30h5HI5nJ1dGv5AmgFDuYi83O0BAJl5JQzlRERELYCx7Fdy82Y+XnvtLQwdOlyn/d13/wVLy/9NYxkxYjQ+/PADxMfvwowZsyGTye7br1qtxv/931cwN78bQe3s7LFy5XJcuXIZHTp0fKhrq6qqEB29Fr6+fvjkkzXa8zp27IQlS95lKKd7ay23Qmu5JTJzi8UuhYiIiOrph99ycPxsToOuTc8ugrpad2GJSrUGGxIv4LvT2Xr11bdbW/Txa9ugOh7EysoKERG1d0v/ayAvKytFZWUV/P0DsXdvHDIzM9Cpk/d9+x0y5CltWAYAf/+7+9RkZ19/YCh/0LVpaedRVFSEOXOe1jkvPDwCq1Z9fN++DQFDuciUrnJk5pWIXQYRERE1g78H8ge1i8XZ2UUn2Na4ciUd69atRWrqzygtLdU5Vlr64DxTMw2mhlxuBwBQqR68EtqDrs3NvftFyd3dQ+c8c3NztG3bNF9eGhNDuciUCjnOpN9ERWU1LGVmYpdDRERED9DHr+Ej1K+s+eGe+5Usmhj0sKU1mr+OiNdQqVSYN28mWrWyxfTpUXBzc4dMJsOlS2lYu3Y1NBpNHT3pkkrrzjr1WaH7Ya41Blx9RWRKVzkEAbh2g6PlREREpm5kPy/IzHXjl8xcipH9vESqqP5+/fUXFBUV4Y033sGYMePRp88T6NGjl3bEWmwKxd0vSn9fkUWtViMnp2HTjZoTQ7nIlIq7u5FmcF45ERGRyevtq8CUwZ21O3k72lkazX4lUund2PjXkemqqirEx+8SqyQdnTs/Cnt7eyQkxEOtVmvbDx06AJXK8HMWp6+IzMFWBjsbGTLzuKskERFRS9DbV2EUIfzv/Py6QS63w5Il72L06LGQSCQ4eDARhjJ7xMLCAs8+OxMrVnyI+fPnICRkAHJycrB//z64ublDIpGIXeJ9caRcZBKJBJ4KOTJzGcqJiIjIcNnbO2DZshVwdHTCunVrsW3bFnTv3gtz5rwgdmlao0aNxfz5C5Gbm4PPPluJM2d+xdKlH8PWVg6ZzFLs8u5LIpjK7PiHVFBQAo2meR+Fs7Mc+fkqxH13BYknMrHmpSchs2i+lz1r7i8Wse9vKDUQEZHhys3NhEKhFLsMeggajQZDh4ajX78QLFr0ZpPe60F/L1KpBI6OtnUfa6qiqP48FXJoBAHX8vmyJxEREVFDVVTUXtnmwIFvUFxchMDAYBEqqj/OKTcASte7L3tezVXB6xF7kashIiIiMk5nz57G2rWr0b9/KOzs7HHpUhq++SYBHTp4ISQkTOzy7ouh3AC0sbOErbUFMjivnIiIiKjBHnnEDU5OzoiN3YHi4iLY2dkjImIIoqLmwsLCQuzy7ouh3ABIJBIoFXKuwEJERET0ENzc3LFs2Qqxy2gQzik3EJ4KOa7nl6JK/eDdsIiIiIjItDCUGwilqxzVGgHXb/JlTyIiIqKWhqHcQPxvZ09OYSEiIiJqaRjKDYSTvRVsrMxxlaGciIiIqMVhKDcQEokE7VzlHCknIiIiaoEYyg2IUiFHVn4J1NV82ZOIiIioJWEoNyCeCjnU1QKyb5aKXQoRERERNSOGcgNSs7NnJqewEBERkZFITNyHvn27IycnW9s2evQwLFnyboOufVipqafQt293pKaearQ+mwNDuQFxbm0Na0szZHATISIiImoir766AGFhfXHnzp17nvPSS3MxaFA/VFRUNGNl+jl8+CB27twqdhmNhqHcgEglErRzkXMFFiIiImoy4eGDUF5ejuPHj9V5/PbtW/jll5/x5JMhsLS0bNA9tm7djUWL3nyYMh/oyJFvsXPntlrtAQFBOHLkBwQEBDXp/RsbQ7mBUSrkuHajBNUavuxJREREje+JJ/rD2roVDh8+WOfxpKTDqK6uxsCBEQ2+h0wmg7m5eYOvfxhSqRSWlpaQSo0r5orztOielAo5KtUa5Nwsg7uLrdjlEBERkYmxsrLCE0/0Q3LyYRQXF8POzk7n+OHDB+Ho6AgPDyWWL1+KX35JQV5eHqysrBAU1B3PP/8i2rZ95L73GD16GAIDg/HGG+9q265cSccnn3yIc+d+g729PYYPHwknJ+da137//VEkJMTj0qWLKC4ugrOzCyIjh2HSpGkwMzMDAMydOxOnT6cCAPr27Q4AUCjaIjZ2H1JTT+GFF6KwatXnCArqru33yJFvsWXLRmRmZqBVKxv06fMEZs9+AQ4ODtpz5s6diZKSErz99mJ8/PEyXLjwO+RyOzzzzDhMnDhFvwetJ4ZyA+P53509M/NUDOVEREQmKCU3FQnpB3C7ohCtLR3wlFcEeiqad6pFeHgEvv12P44ePYKnnnpa256bm4Nz585i9OhxuHDhd5w7dxZhYYPg7OyCnJxs7NmzG/PmzcKWLbtgZWVV7/sVFNzECy9EQaPR4B//mAIrK2skJMTXOT0mMfFrWFu3wtixE9GqlTV++eUUoqM/R2lpKZ5//kUAwJQpz+LOnTvIy8vBvHkvAQCsrVvd8/6JifvwwQfvwdfXD7Nnv4AbN/Kwe/cOXLjwO9at26RTR3FxEV5++QWEhAzAgAEDkZx8GGvXrkaHDh3Ru3efen9mfTGUGxjX1q1gaWGGjFwV+vi1FbscIiIiakQpuanYmrYbVZoqAMDtikJsTdsNAM0azHv06AUHh9Y4fPigTig/fPggBEFAePggeHl1REhImM51ffo8iaioaTh69AgiIobU+34xMV+hqKgQ0dGb4ePTGQAwePBQjB//dK1z3333X7C0/F/gHzFiND788APEx+/CjBmzIZPJ0KPHY4iL24WiokIMGhR533ur1WqsXbsaHTt6Y/XqLyCTyQAAPj6d8e67b2DfvniMHj1Oe/6NG3l4551/ITz87vSdoUOHY/Toofjmm70M5S2JVCpBO1dbZHIFFiIiIoN0MucXnMj5uUHX/ll0FWpBrdNWpalCzIVY/Jidoldfvdv2QK+2wQ2qw9zcHKGhYdizZzdu3rwJJycnAMDhw9/C3d0Djz7aVed8tVqN0tISuLt7wNZWjkuX0vQK5SdO/AA/P39tIAeA1q1bIzx8MOLjd+mc+9dAXlZWisrKKvj7B2Lv3jhkZmagUydvvT5rWtp53L59Sxvoa4SGhuOzz1bixx9/0Anltra2CAsbpP3ZwsICXbr4Ijv7ul731RdDuQFSusrx3dlsaDQCpFKJ2OUQERFRI/l7IH9Qe1MKD49AXNwuJCV9izFjJiAj409cvnwJ06bNAABUVJRj8+aNSEzch/z8GxAEQXttSUmJXvfKy8uFn59/rfZ27ZS12q5cSce6dWuRmvozSkt1N1QsLdXvvsDdKTl13UsqlcLd3QN5eTk67S4urpBIdPOXXG6H9PTLet9bHwzlBkipkKPyFw1yb5XhEScbscshIiKiv+jVNrjBI9Rv/vABblcU1mpvbemA+UFRD1uaXvz8/NG2rRsOHTqAMWMm4NChAwCgnbaxYsWHSEzch2eeGY+uXf1ga2sLQIJ33/2nTkBvTCqVCvPmzUSrVraYPj0Kbm7ukMlkuHQpDWvXrkOFTwAAACAASURBVIamGVank0rN6mxvqs9cg6HcACkV/9vZk6GciIjIdDzlFaEzpxwALKQWeMqr4csPPoywsIHYvHkDsrKu4ciRb+Hj00U7olwzb3zevAXa8ysqKvQeJQcAV1cFsrKu1Wq/ejVT5+dff/0FRUVFWLLkQ511xuve8bN+swkUirbae/21T0EQkJV1De3be9Wrn6ZmXAs4thBtHVtBZi7lvHIiIiIT01MRhAmdR6G15d1l+FpbOmBC51HNvvpKjYEDBwMAPv10BbKyrumsTV7XiPHu3TtQXV2t93169+6D3347g4sX07Rtt2/fxqFD+3XOq1lb/K+j0lVVVbXmnQOAtbV1vb4gdO78KFq3boM9e2JRVfW/L0PJyUeQn38Djz/edC9v6oMj5QbITCqFh4stMrizJxERkcnpqQgSLYT/Xfv2HdCxozeOH/8OUqkUAwb87wXHxx/vi4MHE2FjYwtPz/b4/fffcOpUCuzt7fW+z4QJU3DwYCJeeul5jB49DpaWVkhIiIera1uUlPyhPc/PrxvkcjssWfIuRo8eC4lEgoMHE1HXzBEfn8749tv9WL36Y3Tu/CisrVuhb98na51nbm6O2bPn4YMP3sO8ebMQFjYQN27kITZ2Bzp08MKwYbVXgBGDqKH87NmziI+Px8mTJ5GdnQ0HBwcEBgZi/vz5UCprT/yvy759+/DVV1/h8uXLkMlk8Pb2xquvvopu3bo1cfVNS6mQ48dzudAIAqQSvuxJRERETWPgwAhcvnwJgYHB2lVYAODFFxdCKpXi0KH9qKiohJ+fPz755DO89NI8ve/h5OSEVau+wIoVy7B580adzYOWLn1fe569vQOWLVuBTz/9BOvWrYVcboeBAweje/eeeOmluTp9Dh8+CpcupSEx8Wvs2LEVCkXbOkM5AERGDoNMJkNMzFf47LOVsLGxQXh4BKKi5tW5VroYJEJTz1q/jxdeeAGpqamIiIiAj48P8vPzERMTg7KyMsTGxsLL6/5zfFasWIHo6Gg89dRTCAoKQllZGdLS0hAWFoYBAwboVUtBQQk0muZ9FM7OcuTn1z0a/v2ZbGzYn4YPZj4GRZt7L4bfVPdvDmLf31BqICIiw5WbmwmFon4DhUQP+nuRSiVwdKx7c0hRR8qnTp2K5cuX66wZGRkZiWHDhmHdunVYunTpPa9NTU3FF198gdWrVyM8PLw5ym1Wf33Zs6lCOREREREZBlFf9AwKCtIJ5ADg6emJTp06IT09/b7Xbtq0CX5+fggPD4dGo6m1jqWxe8TJBuZmUmRyXjkRERGRyTO41VcEQcDNmzfRunXr+5534sQJ+Pn54eOPP0ZwcDCCgoIQGhqKhISEZqq0aZmbSeHhYsMVWIiIiIhaAINbfSUhIQF5eXlYsGDBPc8pKipCYWEhvvnmG5iZmWHhwoVwcHBATEwMXnnlFVhbW5vElBalqxwpF+7uoPX3naWIiIiIyHQYVChPT0/H4sWLERwcjOHDh9/zvLKyMgBAYWEhdu7cCX//u9u2hoeHIzw8HJ999pneofxek+6bmrOz/J7HunZyxtHT2dCYmUHh2DSbCN3v/s1B7PsbSg1ERGSYbtyQwtzc4CYWkIGSSqUNzhUGE8rz8/Mxa9Ys2NvbY+XKldrF4+tSs3SNu7u7NpADgEwmw6BBg7Bp0yaUlpbCxqb+QdbQVl8BgDY2FgCA1PO56NHZpdnv39TEvr+h1EBERIZLo9FArW76rd3JNGg0mvvmivutvmIQX/1UKhVmzJgBlUqF6OhoODs73/d8BwcHyGQynbU0azg5OUEQhAZtAWto3JxsYSaV8GVPIiIiIhMn+kh5RUUFoqKikJGRgY0bN6JDhw4PvEYqlaJLly7Iy8urdSw3NxdmZmYN2m3K0FiYS+HmbIPM3GKxSyEiImqx+G4X1cfDbv0j6kh5dXU15s+fj9OnT2PlypUICAio87zs7OxaSyRGREQgJycHP/zwg7atpKQE+/fvR2BgIKysrJq09ubiqZAjM6/koX/RREREpD8zM3NUVVWKXQYZgaqqSpiZNXy8W9SR8qVLlyIpKQkhISEoLCzE3r17tcdsbGwQFhYGAFi0aBFSUlJw8eJF7fHx48dj165dmDdvHqZOnQo7Ozvs3r0bKpUKL730UrN/lqaidJXjuzM5KCguh5O9tdjlEBERtSi2tg4oLMyHg4MzLCxkHDGnWgRBQFVVJQoL8yGX339J7/sRNZSnpaUBAJKTk5GcnKxzzM3NTRvK62JtbY1NmzZh2bJl2LJlC8rLy+Hr64sNGzYgODi4SetuTkqFHQAgM7eEoZyIiKiZWVvfXTSiqOgmqqvVIldDhsrMzBxyeWvt30tDSATOiwBgmKuvAECVuhqzP/oOkb3bYeSTXs1+/6Yk9v0NpQYiIiJqGQx+9RW6NwtzMzziZIPMXONfTYaIiIiI6sZQbgQ8FXJk5hbzZU8iIiIiE8VQbgSUCjmKy6pQWMK3v4mIiIhMEUO5EVAq7m7XmsH1yomIiIhMEkO5EfBwsYVEAu7sSURERGSiGMqNgKWFGR5xtGEoJyIiIjJRDOVGop2rHJl5DOVEREREpoih3Eh4KuQoLKlEUUmF2KUQERERUSNjKDcSNS97crSciIiIyPQwlBsJDxdbSABkcF45ERERkclhKDcS1pbmcG3Tii97EhEREZkghnIj4qngy55EREREpoih3Ii0c5XjVnEFisu4sycRERGRKWEoNyKe/33Z8yqnsBARERGZFIZyI9LO9W4o58ueRERERKaFodyItLIyh0tra84rJyIiIjIxDOVGRukq5wosRERERCaGodzIeCrkuFlUjpI7VWKXQkRERESNhKHcyLTjzp5EREREJoeh3MgoXbkCCxEREZGpYSg3MrbWFnCyt+IKLEREREQmhKHcCCm5sycRERGRSWEoN0JKVzlu3L6DsnK12KUQERERUSNgKDdC2p09OVpOREREZBIYyo1QzQosnFdOREREZBoYyo2QXSsZ2thZcqSciIiIyEQwlBsppaucI+VEREREJoKh3EgpFXLk3SrDnQq+7ElERERk7BjKjZTSVQ4BwLUbJWKXQkREREQPiaHcSNWswJLJKSxERERERo+h3EjZ21rC3lbGeeVEREREJoCh3Ih5usq5AgsRERGRCWAoN2JKhRzZBaWoqKwWuxQiIiIieggM5UZMqZBDEIBr+XzZk4iIiMiYMZQbMaUrX/YkIiIiMgXmYt347NmziI+Px8mTJ5GdnQ0HBwcEBgZi/vz5UCqV97129erV+PTTT2u1Ozk54Ycffmiqkg1Oa7kl7FpZMJQTERERGTnRQnl0dDRSU1MREREBHx8f5OfnIyYmBiNGjEBsbCy8vLwe2MfixYthZWWl/fmv/78lkEgkaKfgzp5ERERExk60UD516lQsX74cMplM2xYZGYlhw4Zh3bp1WLp06QP7GDx4MOzs7JqyTIPnqZAj8c+rqFJXw8LcTOxyiIiIiKgBRJtTHhQUpBPIAcDT0xOdOnVCenp6vfoQBAElJSUQBKEpSjQKSlc5NIKAazdKxS6FiIiIiBrIoF70FAQBN2/eROvWret1fv/+/REcHIzg4GC8/vrrKCwsbOIKDY+yZmdPrldOREREZLREm75Sl4SEBOTl5WHBggX3Pc/Ozg6TJk2Cv78/LCws8NNPP2HHjh04f/48du3aVWsE3pQ52lnBxsocmbnFANzELoeIiIiIGkAiGMjcj/T0dIwZMwY+Pj7YsmULpFL9BvFjYmKwePFivP/++xgzZkwTVWmY3vriR6jKKvHJgv5il0JEREREDWAQoTw/Px/jx4+HRqPBjh074OzsrHcfGo0GQUFBCAkJwYoVK/S+vqCgBBpN8z4KZ2c58vMfftrJrqOX8W3KNax5qR8szOv/Zaax7t9QYt/fUGogIiKilkEqlcDR0bbuY81cSy0qlQozZsyASqVCdHR0gwI5AEilUri6uqKoqKiRKzR8ngo7VGsEZN/ky55ERERExkjUUF5RUYGoqChkZGTgiy++QIcOHRrcV1VVFXJycur9kqgpUbre/caVkVssciVERERE1BCihfLq6mrMnz8fp0+fxsqVKxEQEFDnednZ2bWWSLx161at89avX4+Kigo88cQTTVKvIXN2sIa1pTl39iQiIiIyUqKtvrJ06VIkJSUhJCQEhYWF2Lt3r/aYjY0NwsLCAACLFi1CSkoKLl68qD0eEhKCyMhIeHt7QyaT4eTJkzh48CCCg4MxdOjQZv8sYpNIJFC62nJZRCIiIiIjJVooT0tLAwAkJycjOTlZ55ibm5s2lNdl2LBhSE1NxYEDB1BVVQU3NzfMmTMHs2bNgrm5Qa3y2Gw8FXY4/EsW1NUamJuJ/qoAEREREelBtAS7efPmBp/3r3/9q7HLMXpKhRzqag2yb5ainatc7HKIiIiISA8cUjUR2p09Oa+ciIiIyOgwlJsIl9bWsJKZcV45ERERkRFiKDcRUokE7VzlHCknIiIiMkIM5SbEUyHHtRslqNZoxC6FiIiIiPTAUG5ClK5yVKo1yCkoE7sUIiIiItIDQ7kJ4cueRERERMaJodyEKNq0gqWFGUM5ERERkZFhKDchUqkEHtzZk4iIiMjoMJSbGKWrHFfzSqDRCGKXQkRERET1xFBuYjwVclRUVSPvNl/2JCIiIjIWDOUmRul692XPDM4rJyIiIjIaDOUmpq1TK1iYS/myJxEREZERYSg3MWZSKTxcbBnKiYiIiIwIQ7kJUirkuHpDBY3Alz2JiIiIjAFDuQlSuspxp6Ia+bfviF0KEREREdUDQ7kJ8qzZ2ZPrlRMREREZBYZyE/SIkw3MzSRcgYWIiIjISDCUmyBzMyncnfmyJxEREZGxYCg3UUqFHFfzVBD4sicRERGRwWMoN1FKhRyl5WrcLCoXuxQiIiIiegCGchNVs7Mnp7AQERERGT6GchPl7mwLM6mEK7AQERERGQGGchNlYS6Fm5MNV2AhIiIiMgIM5SZMqZAjM5cvexIREREZOoZyE6ZUyFFypwq3iivELoWIiIiI7oOh3IQpubMnERERkVFgKDdhHs62kEq4sycRERGRoWMoN2EyCzM84tQKVzlSTkRERGTQGMpNnNJVjgy+7ElERERk0BjKTZxSIUdxaSUKSyrFLoWIiIiI7qFRQrlarcbBgwexc+dO5OfnN0aX1Ei0L3tyXjkRERGRwTLX94Jly5bh5MmT2L17NwBAEARMmzYNp06dgiAIcHBwwM6dO9GuXbtGL5b0185FDgmAjNxiBHRyErscIiIiIqqD3iPl33//Pbp37679OSkpCT///DOmT5+Ojz76CADw5ZdfNl6F9FAsZWZQOLbC1bwSsUshIiIionvQe6Q8NzcXSqVS+3NycjLc3d2xcOFCAMAff/yBffv2NV6F9NA8FXJcyLwtdhlEREREdA96j5RXVVXB3Px/Wf7kyZN4/PHHtT97eHhwXrmBUbrKUVhSiaIS7uxJREREZIj0DuUKhQK//vorgLuj4teuXUOPHj20xwsKCtCqVavGq5AeGnf2JCIiIjJsek9fGTJkCNasWYNbt27hjz/+gK2tLfr166c9fuHChXq95Hn27FnEx8fj5MmTyM7OhoODAwIDAzF//nyd6TH1MWPGDHz33XeYPHky3njjDX0/kslr5/q/FVi6efFlTyIiIiJDo3conzVrFnJycnDkyBHY2triP//5D+zs7AAAKpUKSUlJmDp16gP7iY6ORmpqKiIiIuDj44P8/HzExMRgxIgRiI2NhZeXV73qOXr0KE6dOqXvx2hRrC3N4dqmFTK4LCIRERGRQdI7lMtkMnzwwQd1HrOxscHx48dhZWX1wH6mTp2K5cuXQyaTadsiIyMxbNgwrFu3DkuXLn1gH5WVlfj3v/+N6dOnY/Xq1fX/EC2Q0tUW6deLxC6DiIiIiOrQqDt6qtVqyOVyWFhYPPDcoKAgnUAOAJ6enujUqRPS09Prdb9NmzahvLwc06dPb1C9LYmnwg4FxRVQlXFnTyIiIiJDo3coP3bsWK1R6ZiYGAQFBSEgIAAvv/wyqqqqGlSMIAi4efMmWrdu/cBz8/PzsWbNGixYsADW1tYNul9LonS1BcCXPYmIiIgMkd6hfP369bhy5Yr25/T0dHzwwQdwcXHB448/jsTERMTExDSomISEBOTl5WHw4MEPPPfjjz9G+/btMXz48Abdq6XRrsDCeeVEREREBkfvOeVXrlzRWW0lMTERlpaWiI2Nha2tLV5++WXs2bOnXi97/lV6ejoWL16M4ODgBwbts2fPYs+ePdi8eTMkEom+H6FOjo62jdKPvpyd5c12L4VjK+TeLte5Z3Pevy5i399QaiAiIqKWTe9QXlRUpDO95Mcff8Rjjz0GW9u7obZnz544duyYXn3m5+dj1qxZsLe3x8qVKyGV3nsAXxAELFmyBAMHDkT37t31Lf+eCgpKoNEIjdZffTg7y5Gf33wj1+7OtriYeUt7z+a+/9+JfX9DqYGIiIhaBqlUcs+BYL2nr7Ru3RrZ2dkAgJKSEvz222864VitVqO6urre/alUKsyYMQMqlQrR0dFwdna+7/mHDh3C2bNnMX78eGRlZWn/V1NPVlYWysvL9f1YLYLS1RY3i8pRWt6wOf9ERERE1DT0HikPCAjA9u3b0bFjR3z33Xeorq7Gk08+qT2emZkJFxeXevVVUVGBqKgoZGRkYOPGjejQocMDr8nOzoZGo8GUKVNqHYuLi0NcXBzWrVunUxPd5am4u558Zq4Kj3q2EbkaIiIiIqqhdyh/4YUXMHnyZMyfPx8A8PTTT6Njx44A7k4tOXz4MHr16vXAfqqrqzF//nycPn0aa9asQUBAQJ3nZWdn486dO9rNhEJDQ+Hu7l7rvOeffx4hISEYPXo0fH199f1YLYL2Zc88hnIiIiIiQ6J3KO/YsSMSExORmpoKuVyOHj16aI8VFxdjypQp9QrlS5cuRVJSEkJCQlBYWIi9e/dqj9nY2CAsLAwAsGjRIqSkpODixYsAgHbt2qFdu3Z19unh4aG9jmqztbaAo50VV2AhIiIiMjB6h3IAcHBwQGhoaK12e3v7OqeV1CUtLQ0AkJycjOTkZJ1jbm5uDNdNxFMhZygnIiIiMjANCuUAcPXqVRw5cgTXrl0DcHeUesCAAfccxf67zZs3N+p5NSPpdH/tFHL8cikfZeVqsUshIiIiov9qUCj/5JNPsG7dulqrrHz44YeYNWsWXnzxxUYpjhqf53/nlV+7oYLS48E7pxIRERFR09M7lMfGxuLzzz9HYGAgnnvuOXTq1AkA8Mcff2D9+vX4/PPP4eHhgZEjRzZ6sfTwlK53Q3lGrgp9Ra6FiIiIiO7SO5Rv3boV/v7+2Lx5M8zN/3d5u3bt0K9fP0ycOBFbtmxhKDdQdjYytJZbIjOP88qJiIiIDIXemwelp6cjMjJSJ5DXMDc3R2RkJNLT0xulOGoaSle+7ElERERkSPQO5RYWFigrK7vn8dLSUlhYWDxUUdS0PBVy5BaU4U4FX/YkIiIiMgR6h3I/Pz/s2LEDN2/erHWsoKAAO3fuhL+/f6MUR02jnUIOAcCV60Vil0JEREREaMCc8jlz5mDq1KmIjIzEqFGjtLt5Xr58GXFxcSgtLcXy5csbvVBqPDUrsKRfL4RLZxeRqyEiIiIiiSAIgr4XJSUl4f3330dOTo5O+yOPPIK3334b/fv3b6z6mk1BQQk0Gr0fxUNxdpYjP7/553af+D0X0V+fhyAAjnaWGNnPC719Fc1eh1if39BqICIiopZBKpXA0dG2zmMNWqc8NDQU/fv3x7lz55CVlQXg7uZBvr6+2LlzJyIjI5GYmNjwiqnJnPg9F1/tT0PNV7GC4gp8tf/u7qpiBHMiIiIieogdPaVSKbp164Zu3brptN++fRt//vnnQxdGTSPuWDoq1Rqdtkq1BnHH0hnKiYiIiESi94ueZNwKiiv0aiciIiKipsdQ3sI42lnq1U5ERERETY+hvIUZ2c8LMvPav/ZBvdqJUA0RERERAQzlLU5vXwWmDO4MRztLSADY28pgJpXg5ws3UK3RPPB6IiIiImp89XrRc8OGDfXuMDU1tcHFUPPo7atAb1+FdjnAn37PxZf7ziPuuyt4pn9HscsjIiIianHqFcr/85//6NWpRCJpUDEkjsd8FbiUVYT9P11FJ3cHBHR0ErskIiIiohalXqF806ZNTV0HiWz8gI74M7sY678+j3em9oCTg7XYJRERERG1GPUK5T179mzqOkhkFuZmmP10V7y34Wes3XsOr00MhkUdL4QSERERUeNj6iItFwdrPBvZBX/mqLAz+bLY5RARERG1GAzlpCPYxxkDe3jgyC9Z+DnthtjlEBEREbUIDOVUy+j+XvBys8OGxAvIvVUmdjlEREREJo+hnGoxN5Ni9vCuMDeTYk38OVRWVYtdEhEREZFJYyinOrWxs8KMYY8iK78EMYcuiV0OERERkUljKKd78uvgiKGPK/H92Rz88FuO2OUQERERmSyGcrqvEX07oHM7B2w+eBFZ+SVil0NERERkkhjK6b6kUglmPeULa0tzrIk/h/JKtdglEREREZkchnJ6IHtbS8x6yhd5t8vw1YGLEARB7JKIiIiITApDOdVLZ2VrPP1EB5w8n4ejp7PFLoeIiIjIpDCUU71F9lbCr4Mjth2+hIzcYrHLISIiIjIZDOVUb1KJBDOGPQo7GxnWxJ9DWXmV2CURERERmQSGctKLrbUFooZ3xW1VBdZ/c4Hzy4mIiIgaAUM56a2jmz2eCemIX/+4iW9/viZ2OURERERGj6GcGiS8uzuCvZ0RezQdl7OKxC6HiIiIyKgxlFODSCQSTIvsgjZ2lli79xxUZZVil0RERERktBjKqcFaWZljzgg/qMqqsG7feWg4v5yIiIioQRjK6aEoFXJMCOuEc3/ewjc/ZohdDhEREZFRMhfrxmfPnkV8fDxOnjyJ7OxsODg4IDAwEPPnz4dSqbzvtQkJCYiNjUV6ejqKiorg4uKCXr16Ye7cuXBzc2umT0A1+gU8gktZhdhz/E90dLNHF882YpdEREREZFREC+XR0dFITU1FREQEfHx8kJ+fj5iYGIwYMQKxsbHw8vK657VpaWlwdXVFv379YG9vj+zsbOzcuRNHjx5FQkICnJ2dm/GTkEQiweRBPsjMVeGLfefx7rQecLC1FLssIiIiIqMhEURaaDo1NRVdu3aFTCbTtmVkZGDYsGEYMmQIli5dqld/v//+O0aOHIlXX30V06dP17uegoISaDTN+yicneXIz1c16z2b8v7Xb5bi/a9+RnuFHRaOD4CZ9P6zo8T+/IZSAxEREbUMUqkEjo62dR9r5lq0goKCdAI5AHh6eqJTp05IT0/Xu79HHnkEAFBczO3fxeLmZIPJg3xw8Voh9nz/p9jlEBERERkN0aav1EUQBNy8eROdO3eu1/mFhYWorq5GdnY2PvvsMwBA7969m7JEeoDHu7bFpWtF+OZEJjq526Obl5PYJREREREZPIMK5QkJCcjLy8OCBQvqdf6gQYNQWFgIAHBwcMDbb7+Nxx57rClLpHqYENYJGTnFWLfvPN6d1hOO9lZil0RERERk0ESbU/536enpGDNmDHx8fLBlyxZIHzAfGQB+/vlnlJWV4c8//0RCQgIiIiIwc+bMZqiWHiT7Zgnmf3wM7Vzl+PfzfWFhztU3iYiIiO7FIEJ5fn4+xo8fD41Ggx07djRo9ZRr165h2LBhWLhwIf7xj3/ofT1f9Gx8p9JuYM2ecwjv7oHxYZ2a/f71YQg1EBERUctgkC961lCpVJgxYwZUKhWio6MbvJyhh4cHfH19sW/fvkaukBqqe2cXhAW749CpaziVdkPscoiIiIgMlqihvKKiAlFRUcjIyMAXX3yBDh06PFR/5eXlUKk46mlIxoR2RPu2dtiw/wLybpeJXQ4RERGRQRItlFdXV2P+/Pk4ffo0Vq5ciYCAgDrPy87OrrVE4q1bt2qdd+7cOaSlpcHX17dJ6qWGMTeTYvYIX0glEqyNP4fKqmqxSyIiIiIyOKKtvrJ06VIkJSUhJCQEhYWF2Lt3r/aYjY0NwsLCAACLFi1CSkoKLl68qD0eEhKCwYMHw9vbG61atcLly5exe/du2NjYYM6cOc3+Wej+nOyt8dzQR7Ey9iy2HfkDUyLqt+QlERERUUshWihPS0sDACQnJyM5OVnnmJubmzaU12XChAk4ceIEDh8+jPLycjg7OyMiIgJz5syBh4dHk9ZNDePf0QmRjymR+FMmvN0d0LurQuySiIiIiAyGQay+Ygi4+krTq9Zo8OG208jILcZbU3ogoItC9JVPxP4dEBERUctxv9VXGMr/i6G8edxWVeCNL0+gslqAoBHQxs4SI/t5obevOCPnYv8OiIiIqOW4Xyg3qB09yfSlXb0NtUbQfgEqKK7AV/vvTmUSK5gTERERiU30dcqpZYk7lg51te6/SFSqNYg7ln6PK4iIiIhMH0M5NauC4gq92omIiIhaAoZyalaOdpZ1tttYcSYVERERtVwM5dSsRvbzgsxc989OIgFKy9X44bcckaoiIiIiEheHJ6lZ1bzMGXcsHbeKK9DGzhLD+7bHyfN5+L/EC7Awl6JnF1eRqyQiIiJqXgzl1Ox6+yrQ21ehsxxhjy6uWLHjNNbtOw+ZuRkCOjmJXCURERFR82EoF0FKbioS0g+gsKIQDpYOeMorAj0VQWKXJSpLCzO8+Iw/lm8/jTV7fsOLo/3h276N2GURERERNQvOKW9mKbmp2Jq2G7crCiEAuF1RiK1pu5GSmyp2aaKztjTHgjH+ULSxwerdZ3HpWqHYJRERERE1C4byZpaQfgBVmiqdtipNFRLSD4hUkWGxtbbAwnEBcLS3wie7zuBKdrHYJRERERE1OYbyZna7ou7R33u1t0R2NjIstXfWjQAAIABJREFUHBcIeSsLfLzjNK7mqcQuiYiIiKhJMZQ3s9aWDnq1t1St5ZZ4ZVwgLGVm+GjHaWTfLBW7JCIiIqImw1DezJ7yioCF1KJWe+c2nUSoxrA5OVjjlfGBkEgkWL79V9y4XSZ2SURERERNgqG8mfVUBGFC51FobekACe6OkCtaueCXvNPILb0hdnkGR9GmFRaOC4C6WsCH207jVnG52CURERERNTqJIAiC2EUYgoKCEmg0zfsoatbpLqwowgcpK9Da0gELu8+FhbR5Vqr86zrhYtDn/hm5xfhw26+wayXDaxODYG9r2ew1EBERET0MqVQCR0fbuo81cy1UBwdLe0zqMgZZJdlISN8vdjkGyVNhhwXPBKCwpBLLt5+GqqxS7JKIiIiIGg1DuYHwc3oUT7o9jqRr3+P3gotil2OQOrrb44VRfsi7fQcf7ziDsvKqB19EREREZAQYyg3I0x2H4BEbBTaf34HiSk6pqEsXzzaYO7IrsvJLsGLXGZRXqsUuiYiIiOihMZQbEJmZBab5TkB5dTk2X9gJjaARuySD1M3LCVHDffFntgqrYs+isqpa7JKIiIiIHgpDuYF5xFaBkR2H4nzBRRzN+kHscgxWsI8Lpg/tgotXC/FZ/DlUqfkFhoiIiIwXQ7kBesKtN/ycHsXey4m4psoWuxyD1dtXgckRPvjtSgG+TPgd1RoGcyIiIjJODOUGSCKR4B+dn4GNRSts+H0rKqq50si99Atww/gB/9/evcdFVef/A3/NfQZmgBkEU0BUNFCxQEsz867FZuat1jSxzXQtL7vY2tb2/e3j0bbb6rZ2tWxL29QttfKSSOXdzdIWL+QlCVQEhVBBYGCAYe6/PwZGRgZEY+YMzOv5eMzjzJw5Z97vGYaZ15z5zDm9cexMKT788ief79aSiIiIqC0wlPsptTwYs/o+hpLaUmw+u13odvzauLtjMHVET/zv9BWs25kL7nqfiIiI2huGcj+WoOuNsd1G4GBxJo6XnBK6Hb82fkh3PHRvLA6cKMaGvWcZzImIiKhd8c2hI+mWPdTzfuRWnMMnOZsQGxIDrTJM6Jb81uRhPWEy27H7aCEUMgmmjogTuiUiIiKiVuGWcj8nFUvxZL8ZsDlsWJu9kbtJbIFIJMJjY3phRFJXfPn9BWw/VCB0S0REREStwlDeDkQGdcK02yfjrP48dhbsF7odvyYSiZD6QDyG9OuMrQfOY9eRQqFbIiIiIrohDl9pJwbdNgDZ5bn4qmA34nW90DM0VuiW/JZYJMLs8X1gttqxce9ZyGVijEyKErotIiIiomZxS3k7IRKJ8Fj8ZGgVoVhzej2MVqPQLfk1iViMeQ/3wx1x4fjPjlx8/+NloVsiIiIiahZDeTuikqrwm34zUGGqxMbcrdzDyA1IJWIsmJyIhFgtVn+ZjaM5JUK3REREROQRQ3k70zM0Fg92H4ejV47j8OUsodvxezKpBIum9kdc11C8n34aJ85dFbolIiIioiYYytuhB7qPQq+wHvj0zFaU1DJk3ohSLkXao3ciOlKNd7f+iOyCcqFbIiIiInIjcnAMBACgrKza54doj4jQoLTUcEvrVtTp8ffDbyBC1QnPDnwGUvHN/2b3l9RvC76uX2204B/rs1CqNyJlUAwOnrqM8ioTdCEKTBkRhyH9bvNZL0RERBR4xGIRwsPVnq/zcS/URrTKMMxIeAQXDIXIOL9L6HbaBbVKhiXTkqCUS5B+8ALKqkxwACirMmHt1zn4/jR/DEpERETC4C4R27HkyP4Y2nUQ9lz8Bgm63kjQ9Ra6Jb8XqlZALGr6WdRstWPTf/NwT9/OEIlEAnTmW9+fvowt3+ShrMqEcH5TQORT/P8LbPz7U3MEC+UnT57E1q1bkZmZieLiYoSFhSE5ORlpaWmIjW15H9y7du3CV199hZMnT6KsrAxdunTBqFGjMH/+fGg0Gh/dA/8wtffDOKcvwLrsjXhx0LNQy4OFbsnv6atNHudXGEz4/dvfISZSjaiIYERHqBETqUbXTsFQyCQ+7tJ7vj99GWu/zoHZ6jw6bMM3BQD4xhAgGAqEw/+/wH7+8e9PLRFsTPnvfvc7ZGVlISUlBfHx8SgtLcUnn3yC2tpabNq0CXFxcc2uO3jwYERGRmLs2LHo2rUrcnNzsXHjRnTv3h2bN2+GQqG46X7a25jyxgoNxVh+dAX6hMdjXv8nWr2lN9DGlDd4buVBlFU1DeZBSikG3h6BotIa/Hy1GmaL80VTBCBCq0JMhDOsx0SqER2hRkSYCmJx+9mqbrLYoDeYsPTjY6iqtTS5PjxEgX/OHypAZ+RL14cCAJBLxXjiVwkBEwq8EQrtDgeMJitq6qyorbPUT53na+uuzT/042W3x75BkFKK2Q/2gVajgE6jgCZYDnEH/NbOH55/vvhQ4HA4YLU5YLLYYLbY6qd2vPHZcY+vv2FqOf78xN1QKSRQyCRe/cZW6A9FQtcXWktjygUL5VlZWUhMTIRcLnfNKygowIQJEzB+/HgsW7as2XUzMzMxePBgt3lffPEFnn/+eSxduhRTpky56X7acygHgH2F32Lz2e2YdvskDI++1+f1b4VQ9VvzpmB3OFCqN6KopAZFpdX1pxqUlNfC0WidqIhgREU4Q3pMRDCiItUICZJ7qOo9drsDVbVmVBhM0BtMqKg2QV9tcl3WVzuvqzVZb3hb8yclondMGEKDfXsffC2Q3hSMJivKDSZUVNWh3GDCp/vOwmiyNVkuSCnFzHG3QxMsR0iQHJogGdQqGaSSjvXTo5b+/wf36YxakxU1riDtHqgbB+2G8w3LGE1WtPQOIhGLEKSUwuAhkDW3fJhaAa3m2kmnUUAbooS2fn6oWn5Lfx9vfSiprbPCUGuGodaCqhozDEYLDDX1l2vNMNSacbaoEjYP77UiEaDVKCCTSiCTiCGTOk9y6bXzzpPzerlM7Lac2/UN55ssJ8Hxc1fx6d6zbn9/mVSMycN7oG+sDmaL3S1IO0/268J1/Txz/WWrDSaz3W0ds8UO+y3GK5HIudcwlUIClVwKpVwCpUIKlWvqvE7pcXptOaVc0uT5IfSHIqHrN/Qg5Ou/X4by5kyZMgUSiQSff/75Ta1XXV2NgQMHYs6cOXjuueduum57D+V2hx3vnfgIZ/V5+ONdv0NX9Y2fYIEayoFr/5Q3u/cVk8WG4qv1Qb1RYG/8RhsaLEd0fVhv2KretVMQZNJrQ2Ba+6JgNFmd4bohZLumZtflympzkxd/sUiEULX82pu6WoEwjfPy5/vPedxS01hnrQq9o8PQOzoUvWPC0Fmr6jBj7f3hTaGtWKx2VBjqUF5lQrmhDmVV18J3eZVzfms+jLUkWClFSLAcmvqg3hDYG+aFBMmc02A5gpTSVm3dbes3RbvdgTqzDXVma/3UBqPZijpT43nO6Z6jRTBZmn4oEQEthmrAeUCyYKUUQfWnYKXMOVXUT5VSBClljZa5dr5h62dz39RpNQosmtofFVUm54co16nOdf76LewiACFquSuk6zRKaEMUrssN5+Uy99ee1jz/HfVb/qtqLTDUmlFVY4HBaL4uZFtcIdxQa2k2hAYrpa7nz9miymYf36H9b4PFaneebHZYLPVTqx1mqx1Wq8113mK1ewz33iQWiaCQiyGXObdmN5zkMnGj803nKeT1l6USrNmR4/GDmVolxZThcTCarTCabKgzWZ3PYXPDeRuMJuflhmlryKRitzD/89VqWG1NHzeFTIJhd3SBSCSCSOQMjiKR8z6LRCKIRXCbupaByG1Zt+vr1218e+t3n0W1sen91wTJ8NT4PgCct4GG20Hj6bXbFsE5RX2dhr8PmizX6LxIhONnS7H123xYBHz9bzeh3OFwYMSIEUhISMAHH3xwU+sWFBTggQcewHPPPYc5c+bcdO32HsoBoMpswN8PvwGNTI3n7loEuUTm0/o3S+j6bdlDZY25PqhXuwJ7cVmN6x9fJAJu0wUhKkINh8OOE+fK3F4YJWIR+vXQIlgpuxa6q00weXjhDVJIEaZRQKuWO6cahTN8qxWuyyFB8maH1jT3ppz6QDxu0wXhbFElzhbpcbao0vXiGRIkcwvpMZHqdrsF9Q/vHkSFoWkoCg2WY8ljSZDXv7HKpc43VW8MUWpNKLXbHdBXm9wCdnnjwG0woarG3OS21SoZdCHOgBYeooQuxBnOdBrn+WWfZKG8mVC45LGka1s5a83XAlmtc4tnQxCrMVo8BlixSAR1kMwtqGsaztcH+gtXDPg686Lbm6JMIsav7umGXlGhHgO1sSFU188z1gfvuvpw4ilkeyIRi1oMchPv64EgxXWBu1G4lrfBb0tu9UOhw+FATZ0VekNDaK9rFNydp3KDCUYPH8LUKhnC1AroQhTIvaj3+HjJZWL0jgp1C9zNPVYqhdTtA9r1H9ha+raluQ8ltzJ8zm53XAvwVjssVpsrsLudbM6t2BabHet25DZ7ewsmJ3oO1nLn5bZ4zWurjQJ2hwOmhg+gDQHe5H7e9b/SKNSfzCtr9jaDFFLYHQ44HM7nW8P5hmlH5svhmy2Fcr/a+0p6ejquXLmCxYsX3/S6q1atgkQiwf333++FztqHELkGqX2mYeWJD7H13JeYFj9J6JYCRmiwHKHBOvTrrnPNs9sduFJRi6LSGldYv3C5CqX6uibr2+wOnMwrR3iIElqNAtERwUjsqbu2lVt9LXwr5L8sGDS88DcXCuOiQpEyuBscDgcul9fiTKHeFdSPnSkF4HwDj+sa6grpcV1DoJT7x8uJw+GAvtqMUr0RJRVGlOhrUVJhdF2uqfO85biyxow/f3i4yXypRAS59NobtLz+zVouvfbG3XB94zDfcF4uc18350I5th+64AqlZVUm/PvLn5CZfRlKudQVuvWGpt+AKOUS6EKU0GkU6NZZ4wrfuhAFdPXPnRv9KHnqiDiPoeCRkXHoEh6MLuE3foxtdjuqjVZXUG+81bSqpiHIm5F/qQqGWrPH4TKNWWx2pB8s8HidVCKu/2q+/ut5uQShwXJ01jovO+dLoFJIry2jaHRd/XxV/Vf5f3zvULOhcOJ9PW5853+hG/3/NUckEkGtcobc6EjPb+gAUGe2Ngnqetf5umY/wJgtdhjNNuhClOh2mwYhjb4F0QTLoFE5P2SpVTLIpLceTqc08/ybMqL535E1RywWOQMzWv+a+OWhgmb//gPjI2+6h5t1q3//64lFIudQFYUUWk3rf0d3qx+KHA4HHPVTu73+siuwO2BvFN6dy3gO969u+AGV1U03JoQGy7Fo6h1udRrWBwC7cybsAOCAh+UABxqdd63XaOoAPtie7fH+eXpMhOA3W8rz8vLw61//GvHx8fj4448hFrf+n3779u1YsmQJ5s2bh2effdaLXbYPa3/YhC/P7MUf73sGd0XdIXQ7dJ2H/7DN41ZGEYD01yb6up2bUlZpRHZ+ObLzy5CdX46C4krYHc43x55dQ9C3R3j9SQdtiNJrfdhsdpTqjbh0tQaXympw6WoNLjdMy2vdvmEQi4AIbRC6dApGl/BgHDj+M2o8fH0aGizHvCl3uMaJXptar7vc8tTcyq22nnQJD0anMBUitCp0CnOeIhpNg1Utf/vVWv89Voh1X/+EqxVGdNKqMOtXfTByYEyb3LYnZosNldVmVFabsPjNb5pd7tWFw6BSOoNGUP20rb+R+e+xQrzz+Qm3cKqQSbDw0Tu9+hj4i9l/24XSCmOT+RFaFf79/3yzUcvXz7/rawfy31/o+y90fX94/rfEL0J5aWkppk+fDrvdjk8//RQRERGtXvfo0aOYPXs2hgwZgpUrV0IiubWtiB1h+EoDi92K5UffQYVJjxcHLUaYItSn9VtL6PpC9dCWX98KzWiyIq+4EmcKK3GuSI/zxVWuLWCRWpVzS3r9sJfbdEGucemtGb5hsdpQoq9DaYURJRW1KHFt+TairLLO7at1qUSMSK0KkfXhNVJ77RQeonQLdt4eU253OGCx2GGyOgO62WKH2Vo/tdjw+mcnml333y+M/sX1/Z0/PP+F/qGXkDrSbypuVSD//QHh77+Q9f3h+e/XY8oNBgNSU1Nx6dIlbNiwAT179mz1ujk5OUhNTUX37t2xdu1aBAUF3XIfHSmUA8DlmhL848hb6BEai4VJczweMEfoUCx0faF68IcXBW+x2uy4cMWAs4VNx6Vr6sely6UiHMu9Covt2v2XSkRI7t0JSrnUFbyvH/etUkgQGRaECK0KnbX14bs+gIdpFDe1+zgh3xT8IZQKqSM//9sLoUMZkZCEfv77bSg3mUyYPXs2Tp8+jTVr1iApKanV6168eBEzZsxAcHAwNmzYAJ1Od+OVWtDRQjkAHCzOxPqczZgU9yDGxY70ef0bEbq+kD0I/aLgKw3j0s8WVeJs/dj0En3Trw4bhATLXVu7O2udwzgatoCrVbIOsQcYhtLAef4TEV3PL0O5zWbDwoULceDAAaxcuRIjRozwuFxxcTGMRqPbwYQahruYTCZs2LAB0dHRv7ifjhjKHQ4HVv/4MU5ePY0lAxcgNsR9vJbQoVjo+v7SQ6CZvWxfs9cFwvANgKGUiChQ+eXeV5YtW4Z9+/Zh1KhR0Ov12LZtm+u64OBgjB07FgDw/PPP4/Dhw8jNvbYbozlz5qCwsBBz5szBsWPHcOzYMdd13bp1Q3Jysu/uiB8TiUR4PGEqLhwuxEen1+OFu38PpdR7P74jao3wEEWzwzcCxZB+tzGEExGRG8FCeU5ODgBg//792L9/v9t1UVFRrlDe0rqrV69uct3kyZMZyhsJkgXhN/2m482sf+HzM+lI7ftroVuiANeWu0QjIiLqKAT/oae/6IjDVxrLOL8TXxfsxZP9ZuCuzkk+r++J0PX9pYdAxOEbREQUiPxy+Ar51q+6j0VO+TlsyNmC7iHd0En1y34YS/RLcPgGERGRu/Z5nGy6aRKxBE/2mw4AWHN6A2z2Wz/ACRERERG1LW4pDyDhKh2mJ0zBR6fX44/f/gUmWx3CFGF4OC4Fg24bIHR7RERERAGLoTzA2B12iCFGna0OAFBh0mN9zmYAYDAnIiIiEgiHrwSY9LwdsMPuNs9ityA9b4dAHRERERERQ3mAqTDpb2o+EREREXkfQ3mA0SrCPM6Xi2UwWps//DkREREReQ9DeYB5OC4FMrHMbZ5EJIbZbsHSw2/ifOUFgTojIiIiClwM5QFm0G0DMCNhKrSKMIjg3HI+s8+vsWTgAgAivJH1Hr7O3wO7w36jmyIiIiKiNsIjetbr6Ef0bE19o9WIjblbcfTKcfQO64kn+j4GrdLzcBdv1BeCP/RAREREgaGlI3pySzm5qKQq/KbvdMzqMw0XDUX4++E3cLzklNBtEREREXV4DOXkRiQSYXCXgXjh7jR0UoVj1Y//wfqczTDbzEK3RkRERNRhMZSTR5FBnfCHgfMxrttIHCzOxD+OvI0iQ7HQbRERERF1SAzl1CypWIpJvR7EoqS5qLUa8c+jK7C/8DvwZwhEREREbYuhnG4oQdcbLw5ajATd7dh0Nh3/OvkRDOZqodsiIiIi6jAYyqlVNHI1nr7jN3j09onIqTiHvx9+Az+VnxG6LSIiIqIOgaGcWk0kEmFk9FD88a5FCJIF4Z3jq7HlXAasdqvQrRERERG1awzldNOi1F3w/F2LcF/UPdh78QBeO/YuSmpLhW6LiIiIqN1iKKdbIpfIMT1+Cub2n4UyYwWWHnkL3186yh+BEhEREd0ChnL6RZIiEvGnQWmI1UTj458+w0en18NoNQrdFhEREVG7wlBOv5hWGYbfJf8WE3qm4IfSU1h6+E2cr7wgdFtERERE7QZDObUJsUiMlO6j8eyAZwCI8EbWe/g6fw/sDrvQrRERERH5PYZyalM9QmPxp0G/x4DIO5CRvwtv//ABKur0QrdFRERE5NcYyqnNqaQq/KbvdMzqMw0XDUX4++E3cLzklNBtEREREfkthnLyCpFIhMFdBuKFu9PQSRWOVT/+B+tzNsNsMwvdGhEREZHfkQrdAHVskUGd8IeB85Fxfhd2X/wv8vT5GNR5AL4t/h/0Jj3CFGF4OC4Fg24bIHSrRERERIJhKCevk4qlmNTrQSToemPVqXVIz9/huq7CpMf6nM0AwGBOREREAYvDV8hnEnS9oZQqm8y32C1Iz9vhYQ0iIiKiwMBQTj6lN1V6nF9h4h5aiIiIKHAxlJNPaRVhzV63PmczyusqfNgNERERkX9gKCefejguBTKxzG2eTCxFfFhvZF46ipe+fxWf5m5tdos6ERERUUfEH3qSTzX8mDM9b0eTva+U11VgZ8E+fFeciUOXjuC+roNxf+wohCpCBO6aiIiIyLtEDofDIXQT/qCsrBp2u28fiogIDUpLDT6t2R7qXzWWY2fBXvzv8jFIRGIMixqC+2NHQSNX+6wHIiIiorYmFosQHu45z3BLOfmdTiodHu/zKMbFjsKOgr3YX/gdvvv5fxgRPRRju42AWh4sdItEREREbYpbyutxS7n/1r9SU4KvCvbg2JUTkEtkGBV9H0Z3G45gWZDPeiAiIiL6pVraUs5QXo+h3P/rX6q5gq/ydyOr5CSUEiVGx9yHUTHDECRT+awHIiIiolvlt8NXTp48ia1btyIzMxPFxcUICwtDcnIy0tLSEBsbe8N1t2zZgpMnT+LMmTOwWCzIzc31UeckhC7BnfFU4kykVF/CV/m78VXBHuwvOogxMcMxMmYoVB4OTERERETUHggaylevXo2srCykpKQgPj4epaWl+OSTTzBp0iRs2rQJcXFxza77zTff4PPPP0d8fDxiYmJw/vx5H3ZOQopSd8Hc/rNQaPgZX+bvRkb+Tuwv/BZju43A8Oh7oZQqhG6RiIiI6KYIOnwlKysLiYmJkMvlrnkFBQWYMGECxo8fj2XLljW77tWrV6FWq6FUKvHKK69g3bp1v2hLOYevtN/6F6oKkZG/C9lluVDLgjEudiSGRw2BXCK/4bpCPwZEREQUOPx2+MqAAQOazOvevTt69+6NvLy8Ftft1KmTt9qidiY2JAYL7nwK5ysv4Mvzu7D13JfYc/EbPBA7GkO7DoZcIrvxjRAREREJyO+O6OlwOHD16lVotVqhW6F2pmdoLBYlz8XiAc+gS1BnbDqbjpe+/we+KToEi90qdHtEREREzfK7/ZSnp6fjypUrWLx4sdCtUDvVK6wHfj9gHs5UnEPG+V347MwX2HVhP1K6j8GQLndBKpbi8OUsj0cV9ZWG+hUmPbSsz/qsz/qsz/qsHxD1W+JXoTwvLw8vv/wyBg4ciIkTJ/q0dnPje7wtIkIjSN1AqB8RkYx7eyfh1JUcfPrjdmzM3YK9hf/FHZ374NuLh2G2WQAAFSY9NuRuQUiICsNiB3mtnwbfXjiMDblbYLaZWZ/1WZ/1WZ/1WT9A6t+I34Ty0tJSzJs3D6GhoXjrrbcgFvt2ZA1/6Nlx63eRROP3dzyN7PJcZJzfhb35B5ssY7aZ8dGxz2A3ev95tzb7M9cLAuuzPuuzPuuzPusLW//jH7YiIaiP1+sD7eDgQQaDAampqbh06RI2bNiAnj173tT63PsK67eWw+HAwv3P+7QmERER+bd3R7/qkzp+u/cVADCZTHj66adRUFCANWvW3HQgJ7oZIpEIWkUYKkz6Jtdp5Gr8tv8TXu/hg1NrYTBXsz7rsz7rsz7rs74f1NcqwrxeuzUEDeU2mw1paWk4fvw4Vq5ciaSkJI/LFRcXw2g0tngwIaLWejguBetzNsNit7jmycQyTOn1EHqGtnwk2bYwpddDrM/6rM/6rM/6rO8n9R+OS/F67daQvPTSSy8JVXzp0qX44osvMGLECMTExCA3N9d1Kioqcm01nz9/Pl599VUsWrTIte7PP/+Mjz/+GEeOHMGRI0dw+fJlSCQSHDlyBAaDAT169LipXoxGM3w9kCc4WIHaWvONF2T9NhWl7gKdUouLVUUw2eqgVYThkdsf9tmvrxvXr2N91md91md91mf9gKgPOL+xDwryfHBDQceUp6am4vDhwx6vi4qKwr59+9yWazxmPDMzE7NmzfK47uTJk1s8GqgnHFMeePX9pQciIiIKDH7/Q09/wFAeePX9pQciIiIKDC2Fcr87oicRERERUaBhKCciIiIiEhhDORERERGRwBjKiYiIiIgExlBORERERCQwhnIiIiIiIoExlBMRERERCYyhnIiIiIhIYFKhG/AXYrEooOqyvn/1QERERB1fS5mDR/QkIiIiIhIYh68QEREREQmMoZyIiIiISGAM5UREREREAmMoJyIiIiISGEM5EREREZHAGMqJiIiIiATGUE5EREREJDCGciIiIiIigTGUExEREREJjKGciIiIiEhgUqEbCDQlJSVYt24dTpw4gR9//BG1tbVYt24dBg8e7PXaJ0+exNatW5GZmYni4mKEhYUhOTkZaWlpiI2N9Xr9U6dO4V//+heys7NRVlYGjUaDhIQELFiwAAMGDPB6fU9WrVqF5cuXIyEhAdu2bROkByIiIiKGch/Lz8/HqlWrEBsbi/j4ePzwww8+q7169WpkZWUhJSUF8fHxKC0txSeffIJJkyZh06ZNiIuL82r9wsJC2Gw2PProo4iIiIDBYMD27dsxc+ZMrFq1CkOHDvVq/euVlpbivffeQ1BQkE/rEhEREV1P5HA4HEI3EUiqq6thsVig1WqxZ88eLFiwwGdbyrOyspCYmAi5XO6aV1BQgAkTJmD8+PFYtmyZ13u4ntFoxNixY5GYmIj333/fp7VfeOEFFBcXw+FwoKqqilvKiYiISDAcU+5jarUaWq1WkNoDBgxwC+QA0L17d/Tu3Rt5eXmC9KRSqaDT6VBVVeXTuidPnkR6ejr+9Kc/+bQuERERkScM5QHO4XDg6tWrPv2gUF1djfLycpw/fx6vv/46zpw5gyFDhvisvsPhwF//+ldMmjTLAcrkAAAHbElEQVQJffr08VldIiIiouZwTHmAS09Px5UrV7B48WKf1XzxxRexc+dOAIBMJsNjjz2Gp59+2mf1v/jiC5w7dw7vvvuuz2oSERERtYShPIDl5eXh5ZdfxsCBAzFx4kSf1V2wYAGmTZuGy5cvY9u2bTCbzbBYLE2G1nhDdXU1XnvtNfz2t79FZGSk1+sRERERtQaHrwSo0tJSzJs3D6GhoXjrrbcgFvvuqRAfH4+hQ4di6tSp+PDDD3H69Gmfje1+7733IJPJ8OSTT/qkHhEREVFrMJQHIIPBgLlz58JgMGD16tWIiIgQrBeZTIYxY8Zg165dqKur82qtkpISrF27FjNmzMDVq1dRVFSEoqIimEwmWCwWFBUVobKy0qs9EBEREXnC4SsBxmQy4emnn0ZBQQHWrFmDnj17Ct0S6urq4HA4UFNTA6VS6bU6ZWVlsFgsWL58OZYvX97k+jFjxmDu3LlYsmSJ13ogIiIi8oShPIDYbDakpaXh+PHjWLlyJZKSknxav7y8HDqdzm1edXU1du7ciS5duiA8PNyr9aOjoz3+uPPNN99EbW0tXnzxRXTv3t2rPRARERF5wlAugJUrVwKAa9/g27Ztw7FjxxASEoKZM2d6re6yZcuwb98+jBo1Cnq93u1gOcHBwRg7dqzXagNAWloaFAoFkpOTERERgUuXLmHLli24fPkyXn/9da/WBgCNRuPxPq5duxYSicTr95+IiIioOTyipwDi4+M9zo+KisK+ffu8Vjc1NRWHDx8WpDYAbNq0Cdu2bcO5c+dQVVUFjUaDpKQkzJ49G4MGDfJq7ZakpqbyiJ5EREQkKIZyIiIiIiKBce8rREREREQCYygnIiIiIhIYQzkRERERkcAYyomIiIiIBMZQTkREREQkMIZyIiIiIiKBMZQTEREREQmMoZyIiASTmpqK0aNHC90GEZHgpEI3QEREbSszMxOzZs1q9nqJRILs7GwfdkRERDfCUE5E1EE99NBDGD58eJP5YjG/JCUi8jcM5UREHVTfvn0xceJEodsgIqJW4OYSIqIAVVRUhPj4eKxYsQIZGRmYMGEC+vfvj5EjR2LFihWwWq1N1snJycGCBQswePBg9O/fHw8++CBWrVoFm83WZNnS0lL87W9/w5gxY5CYmIghQ4bgySefxMGDB5sse+XKFTz77LO4++67ceedd+Kpp55Cfn6+V+43EZE/4pZyIqIOymg0ory8vMl8uVwOtVrturxv3z4UFhbi8ccfR6dOnbBv3z688847KC4uxtKlS13LnTp1CqmpqZBKpa5l9+/fj+XLlyMnJwevvfaaa9mioiJMnz4dZWVlmDhxIhITE2E0GnHixAkcOnQIQ4cOdS1bW1uLmTNn4s4778TixYtRVFSEdevWYf78+cjIyIBEIvHSI0RE5D8YyomIOqgVK1ZgxYoVTeaPHDkS77//vutyTk4ONm3ahH79+gEAZs6ciYULF2LLli2YNm0akpKSAACvvPIKzGYzNm7ciISEBNeyaWlpyMjIwCOPPIIhQ4YAAP7yl7+gpKQEq1evxrBhw9zq2+12t8sVFRV46qmnMHfuXNc8nU6Hf/7znzh06FCT9YmIOiKGciKiDmratGlISUlpMl+n07ldvvfee12BHABEIhHmzJmDPXv2YPfu3UhKSkJZWRl++OEHjBs3zhXIG5Z95plnsGPHDuzevRtDhgyBXq/Ht99+i2HDhnkM1Nf/0FQsFjfZW8w999wDALhw4QJDOREFBIZyIqIOKjY2Fvfee+8Nl4uLi2syr1evXgCAwsJCAM7hKI3nN9azZ0+IxWLXshcvXoTD4UDfvn1b1WdkZCQUCoXbvLCwMACAXq9v1W0QEbV3/KEnEREJqqUx4w6Hw4edEBEJh6GciCjA5eXlNZl37tw5AEBMTAwAIDo62m1+Y+fPn4fdbnct261bN4hEIvz000/eapmIqMNhKCciCnCHDh3C6dOnXZcdDgdWr14NABg7diwAIDw8HMnJydi/fz/OnDnjtuwHH3wAABg3bhwA59CT4cOH48CBAzh06FCTetz6TUTUFMeUExF1UNnZ2di2bZvH6xrCNgAkJCTgiSeewOOPP46IiAjs3bsXhw4dwsSJE5GcnOxa7v/+7/+QmpqKxx9/HDNmzEBERAT279+P7777Dg899JBrzysA8Oc//xnZ2dmYO3cuJk2ahH79+sFkMuHEiROIiorCc8895707TkTUDjGUExF1UBkZGcjIyPB43a5du1xjuUePHo0ePXrg/fffR35+PsLDwzF//nzMnz/fbZ3+/ftj48aNePvtt7FhwwbU1tYiJiYGS5YswezZs92WjYmJwebNm/Huu+/iwIED2LZtG0JCQpCQkIBp06Z55w4TEbVjIge/RyQiCkhFRUUYM2YMFi5ciEWLFgndDhFRQOOYciIiIiIigTGUExEREREJjKGciIiIiEhgHFNORERERCQwbiknIiIiIhIYQzkRERERkcAYyomIiIiIBMZQTkREREQkMIZyIiIiIiKBMZQTEREREQns/wNnUGSNhSpdiQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 864x432 with 1 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"GQoqCn8spNUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yPVID0JUpNWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"R49IYBh2Ehh4"},"execution_count":null,"outputs":[]}]}